from ppfieldbase import PPFieldBase
from dolfin import *
import ufl
try:
    from mpi4py import MPI as MPI4PY
except:
    MPI4PY = None
    warn("mpi4py package not found. This causes much worse WSS performance, in particular for saving.")

from numpy import array, zeros
#parameters["allow_extrapolation"] = True

### Projection from full mesh to boundary mesh

def boundary_entity_map(bdry, mesh):
    gdim = bdry.geometry().dim()
    bdry_cells_to_mesh_facets_map = bdry.entity_map(gdim-1)
    
    # Create map 'parent_cell' from boundary mesh cell index to mesh cell index
    # (each boundary mesh cell is an exterior facet in the original
    # mesh, that only belongs to a single cell)       
    parent_cell = MeshFunction("size_t", bdry, gdim-1)
    
    for i in xrange(len(bdry_cells_to_mesh_facets_map)):
        facet_index = bdry_cells_to_mesh_facets_map[i]
        facet = Facet(mesh, facet_index)
        for cell in cells(facet):
            parent_cell[i] = cell.index()
    
    return parent_cell


def local_mesh_to_boundary_dofmap(bdry, mesh, F, V):
    bmt = bdry.topology()
    
    points_and_normals = [[(0,0,0),(0,0,0)]]*bdry.size_global(2)
    
    # Find midpoints and normals of boundary cells to later determine the intersecting cells (cell dofs) for the full mesh
    for k in range(MPI.num_processes()):
        if MPI.process_number() == k:
            for local_index, global_index in enumerate(bmt.global_indices(2)):
                
                cell = Cell(bdry, local_index)
                p = cell.midpoint()
                n = cell.cell_normal()
                
                points_and_normals[global_index] = [(p[0], p[1], p[2]),(n[0], n[1], n[2])]
                
        MPI.barrier()
        points_and_normals = MPI4PY.COMM_WORLD.bcast(points_and_normals, root=k)
    MPI.barrier()
    
    
    # Find intersecting cell dofs (run along the normal of th boundary cell to guarantee intersection)
    intersected_dofs = [-1]*bmt.size_global(2)
    iop = mesh.intersection_operator()
    for k in range(MPI.num_processes()):
        if MPI.process_number() == k:
            for i, (p, n) in enumerate(points_and_normals):
                deltas = [0, -1e-4, 1e-4]
                for d in deltas:
                    aie = iop.any_intersected_entity(Point(p[0]+d*n[0], p[1]+d*n[1], p[2]+d*n[2]))
                    if aie >= 0:
                        intersected_dofs[i] = F.dofmap().cell_dofs(aie)
                        break
    
        MPI.barrier()
    
        intersected_dofs = MPI4PY.COMM_WORLD.bcast(intersected_dofs, root=k)
        
    MPI.barrier()
    
    # Find the full dofmap from mesh cell dofs to boundary cell dofs
    dofmapping = {}
    for k in range(MPI.num_processes()):
        for local_index, global_index in enumerate(bmt.global_indices(2)):
            bdry_dofs = V.dofmap().cell_dofs(local_index)
            for i, d in enumerate(bdry_dofs):
                dofmapping[intersected_dofs[global_index][i]] = d
        MPI.barrier()
        dofmapping = MPI4PY.COMM_WORLD.bcast(dofmapping, root=k)
    MPI.barrier()
    
    
    # Extract dofmapping specifically required for the current process
    local_dofmapping = {}
    for full_dof, bdry_dof in dofmapping.items():
        if F.dofmap().ownership_range()[0] <= full_dof < F.dofmap().ownership_range()[1]:
            local_dofmapping[full_dof] = bdry_dof

    MPI.barrier()
    return local_dofmapping
   


class WSS(PPFieldBase):
    def __init__(self, **kwargs):
        PPFieldBase.__init__(self, **kwargs)

    def before_first_update(self, u, p, t, timestep, problem):
        try:
            u = as_vector(u)
        except Exception:
            pass
        DG = VectorFunctionSpace(problem.mesh, "DG", 0)
        v = TestFunction(DG)
        
        scaling = 1/FacetArea(problem.mesh)
        n = FacetNormal(problem.mesh)

        mu = problem.params.mu
        
        sigma = mu*(grad(u) + grad(u).T)
        T = -sigma*n
        Tn = inner(T,n)
        Tt = T-Tn*n

        self.stress = scaling*dot(v, Tt)*ds
        self.tau = Function(DG)     

        
        # Determine if it is possible to save on boundary only
        # Currently there are bugs concerning FunctionSpace on BoundaryMesh in parallel
        # (See https://bitbucket.org/fenics-project/dolfin/issue/14/functionspace-on-boundarymesh-does-not)
        self.bdry = None
        if MPI4PY:
            if MPI.num_processes() == 1:
                try:
                    self.bdry = Mesh(problem.params.boundary_mesh_file)
                except:
                    self.bdry = BoundaryMesh(problem.mesh, 'exterior')
            else:
                try:
                    self.bdry = Mesh(problem.params.boundary_mesh_file)
                except Exception:
                    '''
                    #TODO: Print warning
                    warn("Unable to load boundary mesh (parameter: problem.params['boundary_mesh_file']) or mapping (parameter: problem.params['boundary_entity_map_file']).\
                                     The latter can be calculate from function boundary_entity_map in utils.py. \
                                     This is HIGHLY encouraged for vast improvements in the WSS calculations.")
                    '''


        
                
        if not (self.bdry is None):
            self.DG_boundary = VectorFunctionSpace(self.bdry, "DG", 0)            
            self.tau_boundary = Function(self.DG_boundary)
            
            self.local_dofmapping = local_mesh_to_boundary_dofmap(self.bdry, problem.mesh, DG, self.DG_boundary)
            
            self.full_vector = zeros(self.DG_boundary.dim())

        
    def update(self, u, p, t, timestep, problem):
        
        # Logic goes here
        assemble(self.stress, tensor=self.tau.vector())

        # TODO: Save tau on boundayr only. LOTS of potential for saving time and disk space.
        if not (self.bdry is None):
            for mesh_dof, bdry_dof in self.local_dofmapping.items():
                self.full_vector[bdry_dof] = self.tau.vector()[mesh_dof]
            
            self.tau_boundary.vector()[self.local_dofmapping.values()] = self.full_vector[self.local_dofmapping.values()]
            

            self.set_data(t, timestep, self.tau_boundary)

        else:
            self.set_data(t, timestep, self.tau)
    
    def after_last_update(self, u, p, t, timestep, problem):
        pass
    
