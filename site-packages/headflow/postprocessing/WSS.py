
from .PPField import PPField
from ..core.utils import headflow_print, headflow_warning

from dolfin import *
from numpy import array, zeros

### Projection from full mesh to boundary mesh
def boundary_entity_map(bdry, mesh):
    gdim = bdry.geometry().dim()
    bdry_cells_to_mesh_facets_map = bdry.entity_map(gdim-1)

    # Create map 'parent_cell' from boundary mesh cell index to mesh cell index
    # (each boundary mesh cell is an exterior facet in the original
    # mesh, that only belongs to a single cell)
    parent_cell = FacetFunction("size_t", bdry)

    for i in xrange(len(bdry_cells_to_mesh_facets_map)):
        facet_index = bdry_cells_to_mesh_facets_map[i]
        facet = Facet(mesh, facet_index)
        for cell in cells(facet):
            parent_cell[i] = cell.index()

    return parent_cell

def local_mesh_to_boundary_dofmap_cpp(F, V):
    global_to_local_dofmapping = '''
    namespace dolfin {
      std::vector<int> global_to_local_dofmapping(boost::shared_ptr<dolfin::FunctionSpace> F, boost::shared_ptr<dolfin::FunctionSpace> V, boost::shared_ptr<const dolfin::GenericDofMap> full_dofmap, boost::shared_ptr<const dolfin::GenericDofMap> bdry_dofmap)
      {
        // Variables to help in synchronization
        int num_processes = dolfin::MPI::num_processes();
        int this_process = dolfin::MPI::process_number();

        // Extract meshes
        const boost::shared_ptr<const Mesh> full_mesh = F->mesh();
        const boost::shared_ptr<const Mesh> bdry_mesh = V->mesh();

        const std::vector<std::size_t> global_indices = bdry_mesh->topology().global_indices(2);

        // Number of cells in boundary mesh
        const int num_bdry_cells = bdry_mesh->size_global(2);

        // Vectors for bdry cell midpoints and normals (many variables because of troubles with synchronization)
        static std::vector<float> midpoints_x(num_bdry_cells);
        static std::vector<float> midpoints_y(num_bdry_cells);
        static std::vector<float> midpoints_z(num_bdry_cells);

        static std::vector<float> normals_x(num_bdry_cells);
        static std::vector<float> normals_y(num_bdry_cells);
        static std::vector<float> normals_z(num_bdry_cells);

        // Run through cells in local boundary mesh and find normals and midpoints
        for(int k=0; k<num_processes; k++) {
            if(k==this_process) {
                for(size_t local_index=0; local_index < global_indices.size(); local_index++) {
                    size_t global_index = global_indices[local_index];

                    const Cell cell = Cell(*bdry_mesh, local_index);
                    Point midpoint = cell.midpoint();
                    Point normal = cell.cell_normal();

                    midpoints_x[global_index] = midpoint.x();
                    midpoints_y[global_index] = midpoint.y();
                    midpoints_z[global_index] = midpoint.z();

                    normals_x[global_index] = normal.x();
                    normals_y[global_index] = normal.y();
                    normals_z[global_index] = normal.z();
                }
            }
            dolfin::MPI::barrier();

            // Broadcast midpoints and normals so they are equal on all processes
            dolfin::MPI::broadcast(midpoints_x, k);
            dolfin::MPI::broadcast(midpoints_y, k);
            dolfin::MPI::broadcast(midpoints_z, k);

            dolfin::MPI::broadcast(normals_x, k);
            dolfin::MPI::broadcast(normals_y, k);
            dolfin::MPI::broadcast(normals_z, k);

        }
        dolfin::MPI::barrier();


        // Static vector for all cell dofs of full mesh intersecting the boundary cells
        static std::vector< std::vector<dolfin::la_index> > intersected_dofs(num_bdry_cells);
        
        // Use BoundingBoxTree to find corresponding full mesh cell to boundary cell
        boost::shared_ptr<BoundingBoxTree> tree = full_mesh->bounding_box_tree();
        

        // Run along the normal direction to find intersection, because midpoints alone may not intersect full mesh because of round-off errors.
        float deltas [] = {0, -0.0001, 0.0001};

        // Find intersection
        for(int k=0; k<num_processes; k++) {
            if(k==this_process) {
                for(size_t global_index=0; global_index < num_bdry_cells; global_index++) {
                    double px = midpoints_x[global_index];
                    double py = midpoints_y[global_index];
                    double pz = midpoints_z[global_index];

                    double nx = normals_x[global_index];
                    double ny = normals_y[global_index];
                    double nz = normals_z[global_index];

                    for(int i=0; i<3; i++) {
                        float delta = deltas[i];
                        Point p(px+delta*nx, py+delta*ny, pz+delta*nz);

                        unsigned int cell_index = tree->compute_first_entity_collision(p, *full_mesh);
                        
                        if (cell_index != std::numeric_limits<unsigned int>::max())
                        {
                            intersected_dofs[global_index] = full_dofmap->cell_dofs(cell_index);
                            break;
                        }

                    }
                }
            }
            dolfin::MPI::barrier();

            // Broadcast intersected dofs to all processes
            dolfin::MPI::broadcast(intersected_dofs, k);
        }


        // Static container for complete mapping between boundary dofs and full mesh dofs
        static std::vector<size_t> full_dofmapping(bdry_dofmap->global_dimension());

        // Find the mapping between boundary dofs and full mesh dofs
        for(int k=0; k<num_processes; k++) {
            for(size_t local_index=0; local_index < global_indices.size(); local_index++) {
                size_t global_index = global_indices[local_index];

                std::vector<dolfin::la_index> bdry_dofs = bdry_dofmap->cell_dofs(local_index);
                for(int i=0; i<bdry_dofs.size(); i++) {
                    full_dofmapping[bdry_dofs[i]] = intersected_dofs[global_index][i];
                }
            }
            dolfin::MPI::barrier();

            // Broadcast full_dofmapping to all processes
            dolfin::MPI::broadcast(full_dofmapping, k);
        }
        dolfin::MPI::barrier();

        /*
        End of synchronized code
        */


        // Count the number of full mesh dofs that exist on current process
        int count = 0;
        std::pair<std::size_t, std::size_t> range = full_dofmap->ownership_range();
        for(int i=0; i<bdry_dofmap->global_dimension(); i++) {
            size_t full_dof = full_dofmapping[i];
            if(full_dof >= range.first && full_dof < range.second) {
                count += 1;
            }
        }

        // Static Array for containing the local part of the full dofmapping
        // First half: full mesh dof, second half: boundary mesh dof
        std::vector<int> local_dofmapping(count*2);

        int index = 0;
        for(int i=0; i<bdry_dofmap->global_dimension(); i++) {
            size_t full_dof = full_dofmapping[i];
            if(full_dof >= range.first && full_dof < range.second) {
                local_dofmapping[index] = full_dof;
                local_dofmapping[index+count] = i;
                index += 1;
            }
        }


        return local_dofmapping;
      }
    }
    '''

    cpp_module = compile_extension_module(global_to_local_dofmapping, additional_system_headers=["dolfin/geometry/MeshPointIntersection.h", "dolfin/geometry/BoundingBoxTree.h"])

    local_dofmapping_flat = cpp_module.global_to_local_dofmapping(F, V, F.dofmap(), V.dofmap())
    #local_dofmapping_flat = local_dofmapping_flat.array()

    local_dofmapping = {}
    N = len(local_dofmapping_flat)/2
    for i in xrange(N):
        local_dofmapping[local_dofmapping_flat[i]] = local_dofmapping_flat[i+N]

    return local_dofmapping

'''
def local_mesh_to_boundary_dofmap(bdry, mesh, F, V):
    from mpi4py import MPI as MPI4PY
    bmt = bdry.topology()

    points_and_normals = [[(0,0,0),(0,0,0)]]*bdry.size_global(2)

    # Find midpoints and normals of boundary cells to later determine the intersecting cells (cell dofs) for the full mesh
    for k in range(MPI.num_processes()):
        if MPI.process_number() == k:
            for local_index, global_index in enumerate(bmt.global_indices(2)):

                cell = Cell(bdry, local_index)
                p = cell.midpoint()
                n = cell.cell_normal()

                points_and_normals[global_index] = [(p[0], p[1], p[2]),(n[0], n[1], n[2])]

        MPI.barrier()
        points_and_normals = MPI4PY.COMM_WORLD.bcast(points_and_normals, root=k)
    MPI.barrier()


    # Find intersecting cell dofs (run along the normal of th boundary cell to guarantee intersection)
    intersected_dofs = [-1]*bmt.size_global(2)
    iop = mesh.intersection_operator()
    for k in range(MPI.num_processes()):
        if MPI.process_number() == k:
            for i, (p, n) in enumerate(points_and_normals):
                deltas = [0, -1e-4, 1e-4]
                for d in deltas:
                    aie = iop.any_intersected_entity(Point(p[0]+d*n[0], p[1]+d*n[1], p[2]+d*n[2]))
                    if aie >= 0:
                        intersected_dofs[i] = F.dofmap().cell_dofs(aie)
                        break

        MPI.barrier()

        intersected_dofs = MPI4PY.COMM_WORLD.bcast(intersected_dofs, root=k)

    MPI.barrier()

    # Find the full dofmap from mesh cell dofs to boundary cell dofs
    dofmapping = {}
    for k in range(MPI.num_processes()):
        for local_index, global_index in enumerate(bmt.global_indices(2)):
            bdry_dofs = V.dofmap().cell_dofs(local_index)
            for i, d in enumerate(bdry_dofs):
                dofmapping[intersected_dofs[global_index][i]] = d
        MPI.barrier()
        dofmapping = MPI4PY.COMM_WORLD.bcast(dofmapping, root=k)
    MPI.barrier()


    # Extract dofmapping specifically required for the current process
    local_dofmapping = {}
    for full_dof, bdry_dof in dofmapping.items():
        if F.dofmap().ownership_range()[0] <= full_dof < F.dofmap().ownership_range()[1]:
            local_dofmapping[full_dof] = bdry_dof

    MPI.barrier()
    return local_dofmapping
'''


class WSS(PPField):
    def before_first_compute(self, pp, spaces, problem):
        
        headflow_print("Initiating DG0 VectorFunctionSpace for WSS calculation. This might take some time.")
        
        DG = VectorFunctionSpace(problem.mesh, "DG", 0)
        self.v = TestFunction(DG)

        self.tau = Function(DG, name="WSS")

        # Determine if it is possible to save on boundary only
        # Currently there are bugs concerning FunctionSpace on BoundaryMesh in parallel
        # (See https://bitbucket.org/fenics-project/dolfin/issue/14/functionspace-on-boundarymesh-does-not)
        self.bdry = None
        try:
            self.bdry = Mesh(problem.params.boundary_mesh_file)
        except:
            if MPI.num_processes == 1:
                self.bdry = BoundaryMesh(problem.mesh, 'exterior')
            else:
                #headflow_warning("Unable to load boundary mesh (parameter: problem.params['boundary_mesh_file']) \
                #                 or mapping (parameter: problem.params['boundary_entity_map_file']).\
                #                 The latter can be calculate from function boundary_entity_map in utils.py. \
                #                 This is HIGHLY encouraged for vast improvements in the WSS calculations.")
                headflow_warning("Unable to load boundary mesh (parameter: problem.params['boundary_mesh_file']). \
                                 This is HIGHLY encouraged for vast speed improvements in the WSS calculations and saving.")
        
        if not (self.bdry is None):
            self.DG_boundary = VectorFunctionSpace(self.bdry, "DG", 0)

            self.tau_boundary = Function(self.DG_boundary, name="WSS")

            #self.local_dofmapping = local_mesh_to_boundary_dofmap(self.bdry, problem.mesh, DG, self.DG_boundary)
            self.local_dofmapping = local_mesh_to_boundary_dofmap_cpp(DG, self.DG_boundary)

            self.full_vector = zeros(self.DG_boundary.dim())


    def compute(self, pp, spaces, problem):
        scaling = 1/FacetArea(problem.mesh)
        n = FacetNormal(problem.mesh)
        
        sigma = pp.get("Stress")
        T = -sigma*n
        Tn = inner(T,n)
        Tt = T-Tn*n
        
        tau_form = scaling*dot(self.v, Tt)*ds()

        assemble(tau_form, tensor=self.tau.vector())
        
        # TODO: Save tau on boundary only. LOTS of potential for saving time and disk space.
        if not (self.bdry is None):
            for mesh_dof, bdry_dof in self.local_dofmapping.items():
                self.full_vector[bdry_dof] = self.tau.vector()[mesh_dof]

            self.tau_boundary.vector()[self.local_dofmapping.values()] = self.full_vector[self.local_dofmapping.values()]

            data = self.tau_boundary
        else:
            data = self.tau

        return data
