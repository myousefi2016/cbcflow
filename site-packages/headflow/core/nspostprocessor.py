__author__ = "Oyvind Evju <martinal@simula.no>"
__date__ = "2013-06-25"
__copyright__ = "Copyright (C) 2013 " + __author__
__license__  = "GNU GPL version 3 or any later version"

from .paramdict import ParamDict
from .parameterized import Parameterized
from ..postprocessing import field_classes, PPField
from ..postprocessing import * # TODO: Remove this, want to know what dependencies we have here to avoid spaghetti...
from .utils_pyminifier import minify
from .utils import headflow_warning

from dolfin import Function, MPI, plot, File, project, as_vector

import os, re, inspect
from collections import defaultdict


# Need this for safe .txt file storage
if MPI.num_processes() > 1:
    on_master_node = MPI.process_number() == 0
else:
    on_master_node = 1


# Disable all plotting if we run in parallell
if MPI.num_processes() > 1:
    headflow_warning("Unable to plot dolfin plots in paralell. Disabling.")
    disable_plotting = True
else:
    disable_plotting = False


# Set up pylab if available
if disable_plotting:
    pylab = None
else:
    try:
        import pylab
        pylab.ion()
    except:
        pylab = None
        headflow_warning("Unable to load pylab. Disabling pylab plotting.")


# Enable dolfin plotting if environment variable DISPLAY is set
if disable_plotting:
    dolfin_plotting = False
else:
    if 'DISPLAY' in os.environ:
        dolfin_plotting = True
    else:
        headflow_warning("Did not find display. Disabling dolfin plotting.")
        dolfin_plotting = False


def compute_at_this_time(field, t, timestep): # TODO: Rename
    "Check if field is to be computed at current time"
    # Limit by timestep interval
    s = field.params.start_timestep
    e = field.params.end_timestep
    #if s > timestep or timestep > e:
    if not (s <= timestep <= e):
        return False

    # Limit by time interval
    s = field.params.start_time
    e = field.params.end_time
    #if s > t or t > e:
    eps = 1e-10
    if not (s-eps <= t <= e+eps):
        return False

    # Limit by frequency (accept if no previous data)
    if timestep - field._previous_computetimestep < field.params.stride_timestep:
        return False
    if t - field._previous_computetime < field.params.stride_time:
        return False

    # Accept!
    return True


class DependencyException(Exception):
    def __init__(self, fieldname=None, dependency=None, timestep=None, original_exception_msg=None):
        message = ''
        if fieldname:
            message += "Dependency/dependencies not found for field %s." %fieldname
        if dependency:
            message += "Dependency %s not functioning." %dependency
        if timestep:
            message += "Relative timestep is %d. Are you trying to calculate time-derivatives at t=0?" % (name, timestep)
        if original_exception_msg:
            message += "\nOriginal exception was: " + original_exception_msg
        Exception.__init__(self, message)


class NSPostProcessor(Parameterized):
    def __init__(self, params=None):
        Parameterized.__init__(self, params)

        # Storage of actual fields
        self._fields = {}
        self._sorted_fields_keys = [] # Topological ordering
        self._dependencies = {}

        # Cache of computed values
        self._cache = defaultdict(dict)
        self._cache[0] = {}

        # Caches for file storage and plotting
        self._save_counts = defaultdict(int)
        self._datafile_cache = {}
        self._plot_cache = {}

        # Hack to make these objects available throughout during update...
        self._problem = None
        self._spaces = None

        # Callback to be called with fields where the 'callback' action is enabled 
        # Signature: ppcallback(field, data, t, timestep)
        self._callback = None

    @classmethod
    def default_base_params(cls):
        params = ParamDict(
            casedir="",
            )
        return params

    def _insert_in_sorted_list(self, fieldname):
        # Topological ordering of all fields, so that all dependencies are taken care of

        # If already in list, assuming there's no change to dependencies
        if fieldname in self._sorted_fields_keys:
            return

        # Find largest index of dependencies in sorted list
        max_index = max([-1]+[self._sorted_fields_keys.index(dep[0]) for dep in self._dependencies[fieldname]])

        # Insert item after all its dependencies
        self._sorted_fields_keys.insert(max_index+1, fieldname)

    def find_dependencies(self, field):
        "Read dependencies from source (if available) from calls to PostProcessor.get command"

        # Get source
        s = inspect.getsource(field.compute)
        s = minify(s) # Removes all comments, empty lines etc.

        # Remove comment blocks
        s = s.split("'''")
        s = s[0::2]
        s = ''.join(s)
        s = s.split('"""')
        s = s[0::2]
        s = ''.join(s)
        s = minify(s)

        # Get argument names for the compute function
        args = inspect.getargspec(field.compute)[0]
        self_arg = args[0]
        pp_arg = args[1]

        # Read the code for dependencies
        deps = []
        deps_raw = re.findall(pp_arg+".get\((.+)\)", s)
        for dep in deps_raw:
            dep = dep.replace('"', "'")
            dep = dep.split(',')

            # Append default 0 if dependent timestep not specified
            if len(dep) == 1:
                dep.append(0)
            dep[1] = int(dep[1])

            if "'" in dep[0]:
                # If pp.get('Velocity')
                dep[0] = dep[0].replace("'","")
            else:
                # If pp.get(self.somevariable), get the string hiding at self.somevariable
                dep[0] = eval(dep[0].replace(self_arg, "field", 1))
                # TODO: Test alternative solution without eval (a matter of principle) and with better check:
                #s, n = dep[0].split(".")
                #assert s == self_arg, "Only support accessing strings through self."
                #dep[0] = getattr(field, n)
                # TODO: Possible to get variable in other cases through more introspection? Probably not necessary, just curious.

            # Ignore dependencies that are always available # FIXME: This is not true for dependencies over time!
            skiplist = ("t", "timestep", "Pressure", "Velocity")
            if dep[0] in skiplist:
                continue

            # Append to dependencies
            deps.append(tuple(dep))

        return deps

    def add_field(self, field):
        # Did we get a field name instead of a field?
        if isinstance(field, str):
            if field in self._fields.keys():
                # Field of this name already exists, no need to add it again
                return self._fields[field]
            else:
                # Create a proper field object from known field name
                field = field_classes[field](params=None)

        # Note: If field already exists, replace anyway to overwrite params, this
        # typically happens when a fields has been created implicitly by dependencies.
        # This is a bit unsafe though, the user might add a field twice with different parameters...
        # Check that at least the same name is not used for different field classes:
        assert type(field) == type(self._fields.get(field.name,field))

        # Analyse dependencies of field through magic
        deps = self.find_dependencies(field)
        #print deps

        # Add dependent fields to self._fields (this will add known fields by name)
        for depname in set(d[0] for d in deps) - set(self._fields.keys()):
            self.add_field(depname)

        # Add field to internal data structures
        self._fields[field.name] = field
        self._dependencies[field.name] = deps
        self._insert_in_sorted_list(field.name)

        # Returning the field object is useful for testing
        return field

    def add_fields(self, fields):
        return [self.add_field(field) for field in fields]

    def get(self, name, timestep=0):
        """Get the value of a named field at a particular.

        The timestep is relative to now.
        Values are computed at first request and cached.
        """
        # Check cache
        c = self._cache[timestep]
        data = c.get(name)

        # Cache miss?
        if data is None:
            if timestep == 0:
                # Ensure before_first_compute is always called once initially
                field = self._fields[name]
                if field._compute_count == 0:
                    field.before_first_compute(self, self._spaces, self._problem)

                # Compute value
                data = field.compute(self, self._spaces, self._problem)
                field._compute_count += 1
                c[name] = data
            else:
                # Cannot compute missing value from previous timestep,
                # dependency handling must have failed
                raise DependencyException(name, timestep)

        return data

    def _apply_action(self, action, field, data):
        "Apply some named action to computed field data."
        if not field.name in self._cache[0]:
            error("Field '%s' is not in cache, this should not be possible." % field.name)

        if action == "save":
            self._action_save(field, data)

        elif action == "plot":
            self._action_plot(field, data)

        elif action == "callback":
            self._action_callback(field, data)

        else:
            error("Unknown action %s." % action)

    def _action_callback(self, field, data):
        "Apply the 'callback' action to computed field data."
        if callable(self._callback):
            self._callback(field, data, self.get("t"), self.get("timestep"))

    def _get_save_formats(self, field, data):
        if field.params.save_as == PPField.default_save_as():
            # Determine proper file formats from data type if not specifically provided
            if isinstance(data, Function):
                save_as = ['pvd', 'xml.gz']
            elif isinstance(data, (float, int, list, tuple, dict)):
                save_as = ['txt']
            else:
                error("Unknown data type %s, cannot determine file type automatically." % type(data).__name__)
        else:
            if isinstance(save_as, (list, tuple)):
                save_as = tuple(field.params.save_as)
            else:
                save_as = (field.params.save_as,)
        return save_as

    def _get_casedir(self):
        return self.params.casedir

    def _create_casedir(self):
        casedir = self.params.casedir
        if not os.path.isdir(casedir):
            os.mkdir(casedir)
        return casedir

    def _get_savedir(self, field_name):
        return os.path.join(self.params.casedir, field_name)

    def _create_savedir(self, field_name):
        self._create_casedir()
        savedir = self._get_savedir(field_name)
        if not os.path.isdir(savedir):
            os.mkdir(savedir)
        return savedir

    def _update_metadata_file(self, field_name, data, save_count, save_as, metadata):
        if on_master_node:
            savedir = self._get_savedir(field_name)
            metadata_filename = os.path.join(savedir, 'metadata.txt')

            # Create initial metadata file for first save call, or just append to the existing one
            if save_count == 0:
                # Initially just tell what data type this field has, and which formats we will save to
                metadata_file = open(metadata_filename, 'w')
                metadata_file.write('type=%r\n' % (type(data).__name__,))
                metadata_file.write('saveformats=%r\n' % (save_as,))

                # Then add type specific initial metadata
                if isinstance(data, Function):
                    # It's nice to have element data easily accessible
                    # TODO: Support metadescription of mesh regions? Boundary meshes? Will probably needed that for automated interpretation later.
                    metadata_file.write("element=%r\n" % (data.element(),))
                    metadata_file.write("element_degree=%r\n" % (data.element().degree(),))
                    metadata_file.write("element_family=%r\n" % (data.element().family(),))
                    metadata_file.write("element_value_shape=%r\n" % (data.element().value_shape(),))

                # TODO: Figure out and document what kind of txt data we support
                #elif isinstance(data, dict):
                #    metadata_file.write("keys=%r\n" % (sorted(data.keys()),))

                # Add a separator line between header and per-timestep data
                metadata_file.write('#'*40 + '\n')
                metadata_file.close()

            # Write metadata for this timestep
            assert all(isinstance(md, tuple) and len(md) == 2 for md in metadata)
            sep = "\t" # TODO: Separate with tabs or newlines?
            metadata_file = open(metadata_filename, 'a')
            metadata_file.writelines(sep.join("%s=%r" % (md[0], md[1]) for md in metadata))
            metadata_file.write('\n')
            metadata_file.close()

    def _get_datafile_name(self, field_name, saveformat, save_count):
        # These formats produce a new file each time
        counted_formats = ('xml', 'xml.gz')

        # Make filename, with or without save count in name
        if saveformat in counted_formats:
            filename = "%s%d.%s" % (field_name, save_count, saveformat)
            # If we have a new filename each time, store the name in metadata
            metadata = [('filename', filename)]
        else:
            filename = "%s.%s" % (field_name, saveformat)
            metadata = []
        savedir = self._get_savedir(field_name)
        fullname = os.path.join(savedir, filename)
        return fullname, metadata

    def _update_pvd_file(self, field_name, saveformat, data, save_count, t):
        assert isinstance(data, Function)
        assert saveformat == "pvd"
        fullname, metadata = self._get_datafile_name(field_name, saveformat, save_count)
        key = (field_name, saveformat)
        datafile = self._datafile_cache.get(key)
        if datafile is None:
            datafile = File(fullname)
            self._datafile_cache[key] = datafile
        datafile << data
        return metadata

    def _update_xdmf_file(self, field_name, saveformat, data, save_count, t):
        assert isinstance(data, Function)
        assert saveformat == "xdmf"
        fullname, metadata = self._get_datafile_name(field_name, saveformat, save_count)
        key = (field_name, saveformat)
        datafile = self._datafile_cache.get(key)
        if datafile is None:
            datafile = File(fullname)
            self._datafile_cache[key] = datafile
        datafile << (data, t)
        return metadata
    
    def _update_xml_file(self, field_name, saveformat, data, save_count, t):
        assert saveformat == "xml"
        fullname, metadata = self._get_datafile_name(field_name, saveformat, save_count)
        datafile = File(fullname)
        datafile << data
        return metadata

    def _update_xml_gz_file(self, field_name, saveformat, data, save_count, t):
	assert saveformat == "xml.gz"
	fullname, metadata = self._get_datafile_name(field_name, saveformat, save_count)
        datafile = File(fullname)
        datafile << data
        return metadata

    def _update_txt_file(self, field_name, saveformat, data, save_count, t):
        # TODO: Identify which more well defined data formats we need
	assert saveformat == "txt"
	fullname, metadata = self._get_datafile_name(field_name, saveformat, save_count)
        if on_master_node:
            if save_count == 0:
                datafile = open(fullname, 'w')
            else:
                datafile = open(fullname, 'a')
            datafile.write(str(data))
            datafile.write("\n")
            datafile.close()
        return metadata

    def _action_save(self, field, data):
        "Apply the 'save' action to computed field data."

        field_name = field.name
        save_count = self._save_counts[field_name]

        # Get current time (assuming the cache contains valid 't', 'timestep' at each step)
        t = self.get("t")
        timestep = self.get('timestep')

        # Collect metadata shared between data types
        metadata = [
                    ('save_count', save_count),
                    ('timestep', timestep),
                    ('time', t),
                   ]

        # Create save folder first time
        if save_count == 0:
            self._create_savedir(field_name)

        # Get list of file formats
        save_as = self._get_save_formats(field, data)

        # Rename Functions to get the right name in file (NB! This has the obvious side effect!)
        # TODO: We don't need to cache a distinct Function object like we do for plotting, or?
        if isinstance(data, Function):
            data.rename(field_name, "Function produced by headflow postprocessing.")

        # Write data to file for each filetype
        for saveformat in save_as:
            # Write data to file depending on type
            if saveformat == 'pvd':
                metadata += self._update_pvd_file(field_name, saveformat, data, save_count, t)
            elif saveformat == 'xdmf':
                metadata += self._update_xdmf_file(field_name, saveformat, data, save_count, t)
            elif saveformat == 'xml':
                metadata += self._update_xml_file(field_name, saveformat, data, save_count, t)
            elif saveformat == 'xml.gz':
                metadata += self._update_xml_gz_file(field_name, saveformat, data, save_count, t)
            elif saveformat == 'txt':
                metadata += self._update_txt_file(field_name, saveformat, data, save_count, t)
            else:
                error("Unknown save format %s." % (saveformat,))

        # Write new data to metadata file
        self._update_metadata_file(field_name, data, save_count, save_as, metadata)

        # Update save count
        self._save_counts[field_name] = save_count + 1

    def _action_plot(self, field, data):
        "Apply the 'plot' action to computed field data."
        if disable_plotting:
            return
        if isinstance(data, Function):
            if dolfin_plotting:
                self._plot_dolfin(field.name, data)
        elif isinstance(data, float):
            if pylab:
                self._plot_pylab(field.name, data)
        else:
            headflow_warning("Unable to plot object %s of type %s." % (field.name, type(data)))

    def _plot_dolfin(self, field_name, data):
        # Get current time
        t = self.get("t")
        timestep = self.get('timestep')

        # Plot or re-plot
        plot_object = self._plot_cache.get(field_name)
        if plot_object is None:
            plot_object = plot(data, title=field_name)
            self._plot_cache[field_name] = plot_object
        else:
            plot_object.plot(data)

        # Set title and show
        title = "%s, t=%0.4g, timestep=%d" % (field_name, t, timestep)
        plot_object.parameters["title"] = title

    def _plot_pylab(self, field_name, data):
        # Get current time
        t = self.get("t")
        timestep = self.get('timestep')

        # Values to plot
        x = t
        y = data

        # Plot or re-plot
        plot_data = self._plot_cache.get(field_name)
        if plot_data is None:
            figure_number = len(self._plot_cache)
            pylab.figure(figure_number)

            xdata = [x]
            ydata = [y]
            newmin = min(ydata)
            newmax = max(ydata)

            plot_object, = pylab.plot(xdata, ydata)
            self._plot_cache[field_name] = plot_object, figure_number, newmin, newmax
        else:
            plot_object, figure_number, oldmin, oldmax = plot_data
            pylab.figure(figure_number)

            xdata = list(plot_object.get_xdata())
            ydata = list(plot_object.get_ydata())
            xdata.append(x)
            ydata.append(y)
            newmin = min(ydata)
            newmax = max(ydata)

            # Heuristics to avoid changing axis bit by bit, which results in fluttering plots
            # (Based on gut feeling, feel free to adjust these if you have a use case it doesnt work for)
            if newmin < oldmin:
                # If it has decreased, decrease by at least this factor
                #ymin = min(newmin, oldmin*0.8) # TODO: Negative numbers?
                ymin = newmin
            else:
                ymin = newmin
            if newmax > oldmax:
                # If it has increased, increase by at least this factor
                #ymax = max(newmax, oldmax*1.2) # TODO: Negative numbers?
                ymax = newmax
            else:
                ymax = newmax

            # Need to store min/max for the heuristics to work
            self._plot_cache[field_name] = plot_object, figure_number, ymin, ymax
                
            plot_object.set_xdata(xdata)
            plot_object.set_ydata(ydata)
            pylab.axis([self._problem.params.T0, self._problem.params.T, ymin, ymax])

        # Set title and show
        title = "%s, t=%0.4g, timestep=%d, min=%.2g, max=%.2g" % (field_name, t, timestep, newmin, newmax)
        plot_object.get_axes().set_title(title)
        pylab.xlabel("t")
        pylab.ylabel(field_name)
        pylab.draw()

    def update_all(self, u, p, t, timestep, spaces, problem):
        "Update all PPFields"

        # [martinal] ... don't like this but see the practical need. Should we rather initialize NSPostProcessor with the problem in the first place?
        self._problem = problem
        self._spaces = spaces

        # Push back cache one timestep # FIXME: [martinal] I destroyed this logic in refactoring, restore!
        # [martinal] ... this is suboptimal but maybe sufficient, need to think about it
        for tstep in sorted(self._cache)[:-1]:
            for key in self._cache[tstep].keys():
                self._cache[tstep][key] = self._cache[tstep+1][key]

        # Convert u and p into Functions if they are segregated or mixed
        # TODO: With the subfunction assign feature we could do this faster
        # TODO: Move to self.get function, to project only when needed.
        if not isinstance(u, Function):
            #u = project(as_vector(u), self._spaces.V)
            u = project(u, self._spaces.V)
        if not isinstance(p, Function):
            p = project(p, self._spaces.Q)
                
        # Reset cache for current timestep
        self._cache[0] = {
            "t": t,
            "timestep": timestep,
            "Velocity": u,
            "Pressure": p,
            }

        # Update all required fields for this timestep
        for name in self._sorted_fields_keys:
            field = self._fields[name]

            # TODO: Allow different time params per action? Basically swap if and for here plus some details.
            if compute_at_this_time(field, t, timestep):
                try:
                    data = self.get(name)
                    for action in ["save", "plot", "callback"]:
                        if field.params[action]:
                            self._apply_action(action, field, data)

                except DependencyException as e:
                    headflow_warning(e.message)

                # Store compute times to keep track of compute intervals
                field._previous_computetime = t
                field._previous_computetimestep = timestep

        # Make sure dependencies are ready for next timestep
        # TODO: Make safe for increased timestep
        try:
            dt = t - self.get("t", -1) # Estimate timestep
        except:
            dt = 0 # If no data for previous timestep

        # FIXME: [martinal] This logic does not cover two-level dependencies in time
        for name, dep in self._dependencies.items():
            field = self._fields[name]
            for d in dep:
                # If dependency requires earlier timesteps
                cond1 = (d[1] < 0)

                # FIXME: This check is incorrect, it only checks the endpoint not the interval
                # If field is to be computed in the next abs(d[1]) timesteps
                cond2 = compute_at_this_time(field, t+abs(d[1])*dt, timestep+abs(d[1]))

                if (cond1 and cond2):
                    # Trigger compute if dependency not calculated at this timestep
                    dummy = self.get(d[0])

    def finalize_all(self, u, p, t, timestep, problem): # TODO: Call from somewhere
        "Finalize all PPFields"

        # Update all required fields for this timestep
        for name in self._sorted_fields_keys:
            field = self._fields[name]
            field.after_final_compute(self, problem, FIXME)
